{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "\n",
    "from flask import Flask, request\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data.csv\"\n",
    "latest_pkl = \"latest.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load/Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(file):\n",
    "    df01 = pd.read_csv(file)\n",
    "    df01['Transaction Date'] = pd.to_datetime(df01['Transaction Date'], infer_datetime_format=True)\n",
    "    df01.rename(columns={'Transaction Date':'Transaction_Date'}, inplace=True)\n",
    "\n",
    "    data = df01.drop(['Transaction_Date'], axis=1)\n",
    "    data.index = df01.Transaction_Date\n",
    "\n",
    "    data01 = data[(data[\"ATM Name\"]==1)]\n",
    "    data01 = data01.sort_index()\n",
    "\n",
    "    train = data01.astype(np.float64)\n",
    "    logging.info(\"ETL completed.\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VARMAX_Train_Model(train):\n",
    "    exog = train[['Weekday','Festival Religion','Working Day','Holiday Sequence']]\n",
    "    model= sm.tsa.VARMAX(train[['Total amount Withdrawn','Amount withdrawn XYZ Card']], order=(5,0), trend='c', exog=exog)\n",
    "    model_result = model.fit(maxiter=1000, disp=False)\n",
    "    logging.info(\"Model training completed.\")\n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportModel(model_result):\n",
    "    import pickle \n",
    "    from datetime import datetime\n",
    "    \n",
    "    #latest time now\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    \n",
    "    filename = 'ATM_VARMAX01_' + timestamp + '.pkl'\n",
    "    pickle.dump(model_result, open(filename, 'wb'))\n",
    "    logging.info(\"Model exported.\")\n",
    "    \n",
    "    return filename;\n",
    "\n",
    "def handleNewVersion(latestFileName):\n",
    "    try:\n",
    "        os.remove('latest.pkl')\n",
    "    except:\n",
    "        logging.info(\"Existing latest.pkl not found. Proceeding...\")\n",
    "    \n",
    "    newPath = shutil.copy(latestFileName, 'latest.pkl')\n",
    "    logging.info(\"Latest pkl file updated.\")\n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(Weekday,FestivalReligion,WorkingDay,HolidaySequence):\n",
    "    # Use the loaded pickled model to make predictions \n",
    "    pkl_file = open(latest_pkl, 'rb')\n",
    "    model_result = pickle.load(pkl_file) \n",
    "    params = []\n",
    "#     test01 = test[['Weekday','Festival Religion','Working Day','Holiday Sequence']]\n",
    "#     test01 = [[5.0,4.0,0.0,1.0]]\n",
    "    test01 = [[Weekday,FestivalReligion,WorkingDay,HolidaySequence]]\n",
    "    \n",
    "    exog = test01\n",
    "    pred = model_result.get_forecast(steps=1,exog=exog)\n",
    "    pred_m = pred.predicted_mean\n",
    "    logging.info(\"Prediction task completed.\")\n",
    "    \n",
    "    totalAmount = pred_m[\"Total amount Withdrawn\"] \n",
    "    XYZAmount = pred_m[\"Amount withdrawn XYZ Card\"] \n",
    "    return [totalAmount,XYZAmount]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendNewData(newData):\n",
    "    f= open(file,\"a+\")\n",
    "    if(checkNewLine()):\n",
    "        f.write(newData + '\\n')\n",
    "    else:\n",
    "        f.write('\\n' + newData + '\\n')\n",
    "    f.close()\n",
    "    logging.info(\"Appended new row -> \\n\" + newData)\n",
    "    return 0;\n",
    "\n",
    "def checkNewLine():\n",
    "    outcome = False\n",
    "    with open(file, 'rb+') as f:\n",
    "        f.seek(-1,2)\n",
    "        lastByte = f.read().decode(\"utf-8\")\n",
    "    if(lastByte == '\\n'):\n",
    "        outcome = True\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ETL completed.\n",
      "C:\\Users\\Kavish\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:225: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  ' ignored when e.g. forecasting.', ValueWarning)\n",
      "C:\\Users\\Kavish\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:375: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return matrix[[slice(None)]*(matrix.ndim-1) + [0]]\n",
      "INFO:root:Model training completed.\n",
      "INFO:root:Model exported.\n",
      "INFO:root:Existing latest.pkl not found. Proceeding...\n",
      "INFO:root:Latest pkl file updated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = etl(file)\n",
    "model_output = VARMAX_Train_Model(processed_data)\n",
    "latestFileName = exportModel(model_output)\n",
    "handleNewVersion(latestFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = forecast(1.0,1.0,1.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420290.06333141215"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(output[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_length = 4\n",
    "\n",
    "def validateNewData(data):\n",
    "    valid = False\n",
    "    args = data.split(',')\n",
    "    if(len(args)) == required_length:\n",
    "        valid = True\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask Routes and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure Flask routes\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "  return 'Flask service is up and running!'\n",
    "\n",
    "@app.route('/addrow')\n",
    "def addRow():\n",
    "    data = request.args['data']\n",
    "    appendNewData(data)\n",
    "    return jsonify({\"operation\": \"addrow\"},{\"status\": \"success\"},{\"data\":data}), 200\n",
    "\n",
    "@app.route('/updatemodel')\n",
    "def updateModel():\n",
    "    appendNewData(data)\n",
    "    return jsonify({\"operation\": \"updatemodel\"},{\"status\": \"success\"}), 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
